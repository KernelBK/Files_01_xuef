## lecture 01

Well, it must have something to do with thinking.
It's also about perception, and it's about action.
And therefore we're going to talk about models that are targeted at thinking, perception, and action.

And this should not be strange to you, because model making is what MIT is about.

You run into someone at a bar, or relative asks you what you do at MIT, the right knee jerk reaction is to say, we learned how to build models.

That's what we do at MIT.

We build the models using differential equations.

We build models using probabilities.

We build models using physical and computational simulations.

Whatever we do, we build models.

Even in humanities class, MIT approach is to make models that we can use to explain the past, predict the future, understand the subject, and control the world.

That's what MIT is about.

And that's what this subject is about, too.


And now, our models are models of thinking.
So you might say, if I take this classic will I get smarter?
And the answer is yes.
You will get smarter.
Because you'll have better models of your own thinking, not just the subject matter of the subject, but better models of your own thinking.


So models targeted at thinking, perception, and action.

We know that's not quite enough, because in order to have a model, you have to have representation.

So let's say that artificial intelligence is about representations that support the making of models to facilitate an understanding of thinking, perception, and action.

```

models targeted at
thinking, perception, and action
```

要建立模型，你需要有 representation

```
algorithms enabled by
constraints exposed by
representation that support
models targeted at
thinking, perception, and action
```

### generate and test

生成测试法

once you hava a name for sth, you get power over it.

生成器：不能是冗余的；信息足够的。

符号标记给了我们基于概念的力量



### AI的历史

计算积分的程序

不是计算曲线下的面积，而是基于符号运算。（参见《SICP》）

## lecture02 推理: 目标树

Reasoning: Goal Trees and Problem Solving

```
problem reduction 问题规约

```

积分程序（基于符号运算）（Matlab 内置程序）

思考：我们是如何进行积分运算的？

```
一些基本积分（积分表）
变换法则（提取常数...）
	基本变换
	启发式变换

```

涉及到哪些知识？

知识如何表示？

知识如何使用?

涉及多少知识？

每次当你进入新领域时，你都该问自己以上这些问题。这些属于元知识——即关于知识的知识。

knowledge about knowledge is the real power



## lecture 04 搜索：深度优先、爬山、束搜索

搜索不是针对图的，而是针对选择。

深度优先搜索，传统上我们利用调用栈来做到路径回退。但是我们也可以不这样做。我们可以显式的保存回退路径，把新的回退路径不断加入队列前端。（ANSI CL最短路径示例中中好像就是这么做的）

### 爬山：改进深度优先搜索

起点在中间，目标在右边。我们用眼睛一下子就看出不要往左搜索，但是程序还是会往左。

爬山法改进了这一点，它不会一直往左去。而是优先选择距离近的点。

进行排序后加入队列前端。

!!! 在连续空间中爬山法会遇到问题（多元微积分？？？）

在某一点，探索四个方向，看哪个方向能够导致最大海拔上升。

- 会困在局部极大值
- 山脊处，四面都是往下，但是并没有到达山顶（不明白）

### 束搜索：改进广度优先搜索

我们可以把一层一层的扩展看作是波纹的向前推进，束搜索会设置一个束宽，它会限制波纹的宽度（也就是每层的元素个数），加入束宽是2，那么我们会在每一层选出2个离目标点最近的元素，然后继续向前推进。

keep W best

## lecture 05 搜索：最优、分支限界、A*

```python
对于Python或lisp，队列中保存路径，这对于路径扩展是比较方便的。
队列中的路径已经根据路径长度进行了排序
```

A* ===分支界限+扩展列表+可容许启发式（考虑积累距离+直线距离）

哪个效率高，也取决于你所探索的空间的特性。

可容许启发式在有些情况下可能会出问题！！！









