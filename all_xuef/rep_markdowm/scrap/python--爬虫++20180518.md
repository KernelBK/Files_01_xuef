工业界的逻辑是——效率。

尽快把产品做出来，能赚钱。

# 为什么要会爬虫

本质在于信息收集。

避免随机行走。

收集信息，（爬虫）

评估信息，（大数据分析）

帮助决策。



# 抓取策略

1. 深度优先



2. 广度优先



3. pagerank（给网页打分）

被引用越多，pr值越高

4. 大站优先策略



## 如何去重

1. Hash



2. bloom过滤器



# 爬虫质量标准

分布式

可伸缩性

性能和有效性

质量

新鲜性

更新

可扩展性

# 开始

## 准备

1. 环境（最好linux下）

```bash
pip install lxml
	from lxml import etree
```



## HTTP协议

https  ——> http + ssl 

ssl 会对 header+body 加密——>包

### 无连接，无状态

### request Header

#### cookie

服务器用来标识客户端的小字符串（最大4kb）

setCookie来标识用户

客户端每次访问服务器都会携带cookie

服务器如何判断用户是否登录？ 通过cookie？ spring security内部是使用cookie？



### response Header

```http
Location: http://... 浏览器检查这个属性，根据其值跳转（重定向到这个url）
Set-Cookie: UserID=JohnDoe; Max-Age=3600; Version=1
Status: 200 OK
```



#### HTTP响应状态码

我们在对返回的网页处理前，先要判断是否响应成功。

```http
2xx 成功
3xx 跳转
4xx 客户端错误（404资源不存在或 401无权限）
5xx 服务器错误
```



# 抓取对象类型

## 静态网页



## 动态网页

js ajax 渲染的页面

## webservice

通过 api来获取内容（需要先破解）

给其他网站提供数据服务的（wiki，flicker）



# 选择何种抓取策略

1. 重要的网页离种子站点比较近
2. 万维网的深度并没有那么深，一个网页有很多路径可以到达
3. 宽度优先有利于多爬虫并行合作爬取
4. 深度限制与广度优先相结合（设置 max-depth）



# 不重复抓取策略

## 如何记录抓取历史？

1. 将访问过的url保存到数据库 （效率太低）
2. 用HashSet将访问过的url保存起来。那就只需接近O(1)的代价就可以查到一个URL是否被访问过了。（消耗内存）
3. URL经过MD5或SHA-1等单向哈希后在保存到HashSet或数据库
4. Bit-Map方法。建立一个BitSet，将每个URL经过一个哈希函数映射到某一位。

linux中为某数据流生成摘要（只要文件有任何变化，指纹就会改变）

md5sum file



Redis

分布式系统，将散列映射到多台主机的内存



#### Hash

java中的hashtable是个线性表（数组或者链表），而线性表元素是一个链表（当然，一个首元素节点就代表整个链表）

具有同一个hash值得“两个对象”，不一定是同一个对象（可能hash碰撞）。

但是可以肯定，具有不同hash值得“两个对象”一定是不同的两个对象。

参见《java算法》《effective java》

## 提高效率

1. 评估网站的网页数量（百度或谷歌搜索 site:www.mafengwo.cn）
2. 选择合适的hash算法和空间阈值，降低碰撞几率
3. 选择合适的存储结构和算法

## Bloom Filter

使用多个哈希函数

```bash
pip install pybloomfilter
```



# 网站架构分析

```http
http://www.mafengwo.cn/robots.txt
sitemap
```

各个页面的url是有一定逻辑的

# xpath

python中也可以运行 js 代码

安装了 lxml 后就可以使用 xpath

```python
from lxml import etree

>>> from lxml import etree
>>> s='<a class="hask" href="www.taobao.com">oo</a>'
>>> tr = etree.HTML(s)
>>> tr.xpath('//a')[0].attrib
{'href': 'www.taobao.com', 'class': 'hask'}
```

```python
"//*[@class='external text']"
"//*[contains(@class, 'external')]"
```

and or 运算符

选取 p、span、h1 标签下的内容



# 正则

爬虫常用正则

```python
获取标签下文本 '<th|^>|*>(.*?)</th>'
查找特定类别的链接，例如/wiki/不包含Category目录：

查找商品链接，例如jd的商品外链为7位数组的a标签节点
'/\d{7}.html'
```



